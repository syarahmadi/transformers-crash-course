{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Customizing Transformer Architectures in PyTorch and Hugging Face\n",
        "## Introduction\n",
        "Transformers have become the backbone of modern NLP tasks, offering significant improvements in understanding context and relationships in text. Customizing transformer architectures allows researchers and practitioners to tailor models to specific needs, enhancing performance and efficiency for particular tasks. This advanced tutorial will guide you through customizing transformer architectures using PyTorch and Hugging Face's transformers library.\n",
        "\n",
        "## Setting Up Your Environment\n",
        "To begin, ensure you're using a Google Colab notebook with GPU support for efficient model training and manipulation.\n",
        "\n",
        "## Install Necessary Packages"
      ],
      "metadata": {
        "id": "DpePZbyETrDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision\n",
        "!pip install -q transformers\n",
        "!pip install -q matplotlib"
      ],
      "metadata": {
        "id": "dgZdOtVlT7vF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "e5jLw-ieT_ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel, BertConfig, AutoTokenizer\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qYaRCqLPUCVN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Transformer Architecture\n",
        "A transformer model comprises several key components: embeddings, self-attention mechanisms, and feed-forward neural networks. Customizing these elements can lead to significant improvements or adaptations for specific tasks.\n",
        "\n",
        "## Customizing Components\n",
        "### 1. Custom Embeddings\n",
        "Embeddings transform input tokens into vectors of a specified dimension. Customizing embeddings can adapt how the model interprets input data.\n",
        "\n",
        "Example: Adding Positional Encoding"
      ],
      "metadata": {
        "id": "JVAegPLqUGzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        # Ensure the maximum positions align with BERT's expected maximum\n",
        "        self.max_position_embeddings = config.max_position_embeddings  # typically 512\n",
        "        self.position_embeddings = nn.Embedding(self.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        # Ensure position_ids do not exceed the maximum expected by the model\n",
        "        if seq_length > self.max_position_embeddings:\n",
        "            raise ValueError(f\"Input sequence length ({seq_length}) exceeds maximum allowed length ({self.max_position_embeddings}).\")\n",
        "\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        word_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        embeddings = word_embeddings + position_embeddings\n",
        "        return embeddings\n"
      ],
      "metadata": {
        "id": "advLLDnVUNEi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Custom Self-Attention\n",
        "Self-attention is a mechanism that allows the model to weigh the importance of different parts of the input data.\n",
        "\n",
        "Example: Custom Attention Head"
      ],
      "metadata": {
        "id": "Cl-pzurHT2gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Custom attention logic here\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer"
      ],
      "metadata": {
        "id": "BUcHr8_iUQbX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Custom Feed-Forward Networks\n",
        "Feed-forward networks process the outputs from the attention mechanism. Customizing these can allow for more complex interactions between the model's learned representations.\n",
        "\n",
        "Example: Custom Feed-Forward Layer"
      ],
      "metadata": {
        "id": "-3SGFtwvUTYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomFeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = nn.GELU()  # or any other activation function\n",
        "        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "\n",
        "    def forward(self, attention_output):\n",
        "        intermediate_output = self.dense(attention_output)\n",
        "        intermediate_output = self.intermediate_act_fn(intermediate_output)\n",
        "        layer_output = self.output_dense(intermediate_output)\n",
        "        return layer_output"
      ],
      "metadata": {
        "id": "PrJiAc14USxS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling the Custom Transformer\n",
        "Now, let's put together our custom components into a transformer model."
      ],
      "metadata": {
        "id": "BawRUAQvUZ9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomBertModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CustomBertModel, self).__init__()\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        # Binary classification layer: Maps from hidden size to 1 output\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Pass inputs through BERT model\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Get the last hidden states\n",
        "        last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
        "\n",
        "        # Apply mean pooling to get a fixed size output (aggregate across all tokens)\n",
        "        pooled_output = torch.mean(last_hidden_state, dim=1)  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "        # Pass pooled output through classifier to get the final logit\n",
        "        logit = self.classifier(pooled_output)  # Shape: [batch_size, 1]\n",
        "\n",
        "        return logit.squeeze(-1)  # Shape: [batch_size]"
      ],
      "metadata": {
        "id": "XfXYyYubUYCP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "\n",
        "We create a synthetic dataset for our experiments."
      ],
      "metadata": {
        "id": "S8eXbmQ2U045"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define the correct maximum input ID based on BERT's vocabulary and maximum sequence length\n",
        "max_vocab_size = 30522  # BERT's vocabulary size\n",
        "max_seq_length = 512  # Maximum sequence length for BERT\n",
        "\n",
        "num_samples = 10  # Number of samples in your dataset\n",
        "\n",
        "# Create random data representing your input_ids for the model\n",
        "# Ensure all input IDs are within the vocabulary size range\n",
        "input_ids = torch.randint(low=0, high=max_vocab_size, size=(num_samples, max_seq_length))\n",
        "\n",
        "# Assuming a binary classification task, create random labels\n",
        "labels = torch.randint(low=0, high=2, size=(num_samples,))\n",
        "\n",
        "# Create a TensorDataset and DataLoader\n",
        "dataset = TensorDataset(input_ids, labels)\n",
        "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
      ],
      "metadata": {
        "id": "l11y_QTTU0W-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Custom Model\n",
        "To train your custom model, you'll need a dataset, a loss function, and an optimizer. Here's a simplified training loop:"
      ],
      "metadata": {
        "id": "hRm7Kd3OUfMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have a DataLoader `data_loader` with input_ids and labels\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "model = CustomBertModel(config)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Define the loss function\n",
        "loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):  # number of epochs\n",
        "  # In your training loop\n",
        "  for input_ids, labels in data_loader:\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(input_ids)\n",
        "      outputs = outputs.squeeze(-1)  # Remove the last dimension to match label shape\n",
        "      loss = loss_function(outputs, labels.float())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n"
      ],
      "metadata": {
        "id": "AEk_QzZXUdtj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "Customizing transformers allows you to tailor models to the specific nuances of your task, potentially leading to better performance and more efficient training. Experiment with different configurations and components to find what works best for your specific needs.\n",
        "\n",
        "## Versioning for Reproducibility\n",
        "Ensure reproducibility by noting the version of the key libraries used:"
      ],
      "metadata": {
        "id": "QExKsbETUkMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(\"Torch Version:\", torch.__version__)\n",
        "print(\"Transformers Version:\", transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpSC6xUEUhm2",
        "outputId": "f775b602-a11b-4363-e294-461c04471ddc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch Version: 2.1.0+cu121\n",
            "Transformers Version: 4.35.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFLMQv36Une1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}