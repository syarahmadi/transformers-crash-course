{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Transformer Training with PyTorch and Hugging Face\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Transformer models have reshaped Natural Language Processing (NLP) and are increasingly used for computer vision tasks. However, their massive size and computational demands pose challenges for efficient training. In this tutorial, we'll explore strategies for optimizing Transformer training using PyTorch and Hugging Face while ensuring our code runs smoothly in Google Colab.\n",
        "\n",
        "Here we use the following **efficiency techniques**:\n",
        "\n",
        "* Mixed Precision Training: Automatic mixed precision (FP16) accelerates computations and reduces memory usage without heavily compromising accuracy.\n",
        "* Gradient Scaling: Prevents numerical issues (underflow/overflow) commonly arising in mixed-:precision training.\n",
        "* GPU Utilization: The code assumes GPU availability (device, torch.cuda.amp), significantly boosting performance for Transformer training.\n",
        "\n",
        "## Environment Setup\n",
        "\n",
        "Let's start by setting up a Colab environment and installing packages:"
      ],
      "metadata": {
        "id": "xejg0eEhSFtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch matplotlib datasets"
      ],
      "metadata": {
        "id": "XXmJHT5dSH0I"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries and check versions:"
      ],
      "metadata": {
        "id": "14AtajYKSMoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Torch version: {torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLTpn99SSL7r",
        "outputId": "47856bde-08cf-417a-a80a-198832a21a58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.37.2\n",
            "Torch version: 2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation: Text Classification Example"
      ],
      "metadata": {
        "id": "w2r__5Qh1wOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('glue', 'sst2')  # Let's use the SST-2 sentiment classification task\n",
        "\n",
        "# Preprocessing\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize(batch):\n",
        "  return tokenizer(batch['sentence'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqwZWinpSYc7",
        "outputId": "005c1963-29eb-4b81-bde4-792622076cc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "A-4sTWtK13M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR4OOdVn11mk",
        "outputId": "aaac43f0-924c-44c6-acbd-136222202485"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization Techniques"
      ],
      "metadata": {
        "id": "R72wCP0k1-Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **1. Optimizer: AdamW**\n",
        "# * Employs the AdamW optimizer, a sophisticated optimization algorithm with weight decay for regularization.\n",
        "# * Gathers all trainable weights and biases from the Transformer model using `model.parameters()`.\n",
        "# * Uses a learning rate of 0.00005 (specified by `lr=5e-5`) to control the pace of weight updates.\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# **2. Training and Validation Split**\n",
        "# * Calculates the number of samples to include in the training set (90% of the 'train' subset of your dataset).\n",
        "# Training and validation splits\n",
        "train_size = int(0.9 * len(dataset['train']))\n",
        "train_dataset, val_dataset = dataset['train'].train_test_split(train_size).values()\n",
        "\n",
        "# **3. Custom Collate Function (`collate_fn`)**\n",
        "# * Defines how to assemble a batch of individual samples, handling variable-length sequences in Transformer models.\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to create tensors and transfer to device\"\"\"\n",
        "    input_ids = torch.tensor([example['input_ids'] for example in batch])\n",
        "    attention_mask = torch.tensor([example['attention_mask'] for example in batch])\n",
        "    labels = torch.tensor([example['label'] for example in batch])\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids.to(device),\n",
        "        'attention_mask': attention_mask.to(device),\n",
        "        'labels': labels.to(device)\n",
        "    }\n",
        "\n",
        "# **4. Data Loaders**\n",
        "# * Creates data loaders for efficient batching during training and validation.\n",
        "# * Shuffles training data (`shuffle=True`) for better generalization.\n",
        "# * Employs the `collate_fn` to prepare batches.\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# **5. Learning Rate Scheduler**\n",
        "# * Implements a scheduler to decrease the learning rate linearly over time for stable training.\n",
        "# * Configures the scheduler based on the total number of training steps.\n",
        "num_epochs = 1\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0, # Default value\n",
        "                                            num_training_steps=num_training_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--13Ua-5178U",
        "outputId": "8a1160dc-bd6b-4612-e0cf-23ff38a6b21e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xtzTf_B3ccI",
        "outputId": "ab340bf4-6566-434e-d006-f7df3ee3abd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0, 'input_ids': [101, 4750, 1207, 3318, 5266, 1121, 1103, 22467, 2338, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop with Efficiency"
      ],
      "metadata": {
        "id": "c-s1Oifv2DTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we utilize mixed precision.\n",
        "\n",
        "### Key Benefits of Mixed Precision\n",
        "\n",
        "* Speed: FP16 computations are generally much faster on modern GPUs, leading to significant training acceleration.\n",
        "* Memory: Using FP16 reduces memory footprint, allowing you to train larger models or use larger batch sizes.\n",
        "\n",
        "### Important Considerations\n",
        "\n",
        "* Hardware: Mixed-precision benefits are most pronounced on GPUs with Tensor Cores (NVIDIA architecture).\n",
        "* Stability: Not all models are equally suited for FP16. If you encounter NaN (Not a Number) values or instability, adjust your code or consider a less aggressive mixed-precision approach.\n",
        "\n",
        "In the following part of the code:\n",
        "\n",
        "\n",
        "```\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "```\n",
        "\n",
        "We instantiate a GradScaler object from PyTorch's automatic mixed precision (AMP) package. The GradScaler is crucial for preventing underflow or overflow issues often arising when working with mixed precision.\n",
        "\n",
        "Then we use:\n",
        "\n",
        "```\n",
        "with torch.cuda.amp.autocast():\n",
        "```\n",
        "\n",
        "This context manager enables automatic mixed-precision within its block. Here's how it works:\n",
        "* Casting: It automatically converts model operations and inputs to half-precision (FP16), where possible, for computational speed gains.\n",
        "* Gradient Scaling: During backward propagation, gradients are scaled up to prevent vanishing gradients that can occur in FP16 computations.\n",
        "\n",
        "```\n",
        "outputs = model(**batch)\n",
        "```\n",
        "\n",
        "The forward pass of your Transformer model is executed within the autocast() context. Suitable computations will benefit from the efficiency of FP16.\n",
        "\n",
        "```\n",
        "loss = outputs.loss\n",
        "```\n",
        "\n",
        "The loss calculation is likely performed in full-precision (FP32) for numerical stability.\n",
        "\n",
        "To integrate the scaling during the backward pass:\n",
        "\n",
        "```\n",
        "scaler.scale(loss).backward()  \n",
        "scaler.step(optimizer)\n",
        "scaler.update()\n",
        "```"
      ],
      "metadata": {
        "id": "rdJXRWsF7fOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Progress visualization\n",
        "\n",
        "# WARNING: Turning this to true may take more GPU!\n",
        "evaluate_in_loop = False\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0.0, 0\n",
        "\n",
        "    with tqdm(dataloader, unit=\"batch\") as teval:  # Progress visualization\n",
        "        for batch in teval:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item() * batch['input_ids'].size(0)\n",
        "            predictions = outputs.logits.argmax(dim=-1)\n",
        "            total_correct += (predictions == batch['labels']).sum().item()\n",
        "\n",
        "            # Update progress bar description\n",
        "            teval.set_description(f\"Evaluating: Loss {loss.item():.3f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    accuracy = total_correct / len(dataloader.dataset)\n",
        "\n",
        "    return {'loss': avg_loss, 'accuracy': accuracy}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Mixed precision\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "\n",
        "    if evaluate_in_loop:\n",
        "      # Evaluation\n",
        "      results = evaluate(val_dataloader)\n",
        "      print(f\"Epoch {epoch+1}, Validation Loss: {results['loss']:.3f}, Validation Accuracy: {results['accuracy']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp8SXqNX2Ax1",
        "outputId": "45b5c19e-1d50-4aba-a98b-6eacac54e036"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1684 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "hWvJDzR64QdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_acc = evaluate(val_dataloader)\n",
        "print(f\"Validation Accuracy: {val_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfS7iIuF4QBR",
        "outputId": "e7488681-5b5d-4eeb-e45c-d265da3f6720"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: Loss 0.598:   0%|          | 0/15154 [00:00<?, ?batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: {'loss': 3.943576010961662e-05, 'accuracy': 4.949351634935823e-05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2tNULqIiBmiz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}